{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Drum Transcription Model Training (OaF-style)\n\nThis notebook trains a drum transcription model based on the Onsets and Frames architecture.\n\n**Features:**\n- Multi-class drum detection (kick, snare, hi-hat, toms, cymbals)\n- Velocity prediction for dynamics\n- Ghost note detection\n\n**Dataset:** E-GMD (Expanded Groove MIDI Dataset) - 444 hours of human drum performances\n\n**Requirements:** Colab Pro recommended (~225 GB disk needed for E-GMD)\n\n**Estimated time:** 6-12 hours on A100, 12-24 hours on T4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchaudio librosa pretty_midi mir_eval scikit-learn tqdm wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nEGMD_URL = \"https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0.zip\"\nDATA_DIR = \"/content/e-gmd\"\n\n# Clean up any failed previous attempts\n!rm -rf /content/e-gmd.zip /content/e-gmd-v1.0.0 /content/e-gmd\n!pip cache purge 2>/dev/null\n!rm -rf /root/.cache/pip /tmp/* 2>/dev/null\n\nprint(\"=== Disk space (cleaned) ===\")\n!df -h /content\n\nif not os.path.exists(DATA_DIR):\n    print(\"\\nStreaming E-GMD download + extraction (~90 GB, ~15 min)...\")\n    print(\"(zip is piped directly to extractor â€” never stored on disk)\")\n\n    # bsdtar reads the zip from stdin and extracts on the fly\n    # This avoids needing 2x disk space (zip + extracted)\n    !wget -q --show-progress -O - \"{EGMD_URL}\" | bsdtar -xf - -C /content/\n\n    # Rename extracted directory to expected path\n    !mv /content/e-gmd-v1.0.0 /content/e-gmd 2>/dev/null || true\n\n    print(\"\\n=== Disk after extraction ===\")\n    !df -h /content\nelse:\n    print(\"Dataset already exists, skipping download.\")\n\n# Verify\nprint(f\"\\nWAV files:\")\n!find {DATA_DIR} -name \"*.wav\" | wc -l\nprint(f\"MIDI files:\")\n!find {DATA_DIR} -name \"*.midi\" | wc -l"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'sample_rate': 16000,\n",
    "    'hop_length': 256,\n",
    "    'n_mels': 128,\n",
    "    'n_fft': 2048,\n",
    "    \n",
    "    # Drum classes (General MIDI)\n",
    "    'drum_classes': {\n",
    "        'kick': [35, 36],\n",
    "        'snare': [38, 40],\n",
    "        'hihat_closed': [42, 44],\n",
    "        'hihat_open': [46],\n",
    "        'tom_low': [41, 43, 45],\n",
    "        'tom_high': [47, 48, 50],\n",
    "        'crash': [49, 57],\n",
    "        'ride': [51, 59],\n",
    "    },\n",
    "    'num_classes': 8,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 50,\n",
    "    'chunk_length_sec': 5.0,\n",
    "}\n",
    "\n",
    "# Create class mapping\n",
    "MIDI_TO_CLASS = {}\n",
    "for class_idx, (class_name, midi_notes) in enumerate(CONFIG['drum_classes'].items()):\n",
    "    for midi_note in midi_notes:\n",
    "        MIDI_TO_CLASS[midi_note] = class_idx\n",
    "\n",
    "CLASS_NAMES = list(CONFIG['drum_classes'].keys())\n",
    "print(f\"Drum classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DrumDataset(Dataset):\n    \"\"\"Dataset for drum transcription training using E-GMD.\"\"\"\n    \n    def __init__(self, data_dir, split='train', chunk_length_sec=5.0, sample_rate=16000):\n        self.data_dir = Path(data_dir)\n        self.chunk_length = chunk_length_sec\n        self.sr = sample_rate\n        self.hop_length = CONFIG['hop_length']\n        \n        # Load metadata\n        self.samples = self._load_samples(split)\n        print(f\"Loaded {len(self.samples)} samples for {split}\")\n        \n    def _load_samples(self, split):\n        \"\"\"Load sample paths from E-GMD structure.\"\"\"\n        samples = []\n        \n        # E-GMD structure: drummer/session/file.wav + file.midi\n        # NOTE: E-GMD uses .midi extension, NOT .mid\n        for audio_path in self.data_dir.rglob('*.wav'):\n            midi_path = audio_path.with_suffix('.midi')\n            if midi_path.exists():\n                samples.append({\n                    'audio': str(audio_path),\n                    'midi': str(midi_path)\n                })\n        \n        if len(samples) == 0:\n            # Debug: check if files exist with different extension\n            wav_count = len(list(self.data_dir.rglob('*.wav')))\n            midi_count = len(list(self.data_dir.rglob('*.midi')))\n            mid_count = len(list(self.data_dir.rglob('*.mid')))\n            print(f\"WARNING: 0 matched samples!\")\n            print(f\"  WAV files found: {wav_count}\")\n            print(f\"  .midi files found: {midi_count}\")\n            print(f\"  .mid files found: {mid_count}\")\n            # Show sample filenames\n            for f in list(self.data_dir.rglob('*.wav'))[:3]:\n                print(f\"  Sample WAV: {f.name}\")\n                midi_candidate = f.with_suffix('.midi')\n                print(f\"    Expected MIDI: {midi_candidate.name} (exists: {midi_candidate.exists()})\")\n        \n        # Split 90/10 train/val\n        np.random.seed(42)\n        np.random.shuffle(samples)\n        split_idx = int(len(samples) * 0.9)\n        \n        if split == 'train':\n            return samples[:split_idx]\n        else:\n            return samples[split_idx:]\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load audio\n        audio, sr = torchaudio.load(sample['audio'])\n        if sr != self.sr:\n            audio = torchaudio.functional.resample(audio, sr, self.sr)\n        \n        # Convert to mono\n        if audio.shape[0] > 1:\n            audio = audio.mean(dim=0, keepdim=True)\n        audio = audio.squeeze(0)\n        \n        # Random chunk\n        chunk_samples = int(self.chunk_length * self.sr)\n        if len(audio) > chunk_samples:\n            start = np.random.randint(0, len(audio) - chunk_samples)\n            audio = audio[start:start + chunk_samples]\n            start_time = start / self.sr\n        else:\n            # Pad if too short\n            audio = F.pad(audio, (0, chunk_samples - len(audio)))\n            start_time = 0\n        \n        # Compute mel spectrogram\n        mel_spec = torchaudio.transforms.MelSpectrogram(\n            sample_rate=self.sr,\n            n_fft=CONFIG['n_fft'],\n            hop_length=self.hop_length,\n            n_mels=CONFIG['n_mels']\n        )(audio)\n        mel_spec = torch.log(mel_spec + 1e-8)\n        \n        # Load MIDI and create target\n        import pretty_midi\n        midi = pretty_midi.PrettyMIDI(sample['midi'])\n        \n        # Create onset and frame targets\n        num_frames = mel_spec.shape[-1]\n        onsets = torch.zeros(CONFIG['num_classes'], num_frames)\n        frames = torch.zeros(CONFIG['num_classes'], num_frames)\n        velocities = torch.zeros(CONFIG['num_classes'], num_frames)\n        \n        end_time = start_time + self.chunk_length\n        \n        for instrument in midi.instruments:\n            if instrument.is_drum:\n                for note in instrument.notes:\n                    if start_time <= note.start < end_time:\n                        if note.pitch in MIDI_TO_CLASS:\n                            class_idx = MIDI_TO_CLASS[note.pitch]\n                            onset_frame = int((note.start - start_time) * self.sr / self.hop_length)\n                            end_frame = int((note.end - start_time) * self.sr / self.hop_length)\n                            \n                            if 0 <= onset_frame < num_frames:\n                                onsets[class_idx, onset_frame] = 1\n                                velocities[class_idx, onset_frame] = note.velocity / 127.0\n                                \n                            for f in range(max(0, onset_frame), min(num_frames, end_frame)):\n                                frames[class_idx, f] = 1\n        \n        return {\n            'mel_spec': mel_spec,\n            'onsets': onsets,\n            'frames': frames,\n            'velocities': velocities\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumTranscriptionModel(nn.Module):\n",
    "    \"\"\"Onsets and Frames style model for drum transcription.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_mels=128, num_classes=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Acoustic model (CNN)\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flat_size = 128 * (n_mels // 4)\n",
    "        \n",
    "        # Onset detection branch\n",
    "        self.onset_lstm = nn.LSTM(\n",
    "            self.flat_size, 128, num_layers=2,\n",
    "            batch_first=True, bidirectional=True, dropout=0.3\n",
    "        )\n",
    "        self.onset_fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Frame detection branch (combined with onset)\n",
    "        self.frame_lstm = nn.LSTM(\n",
    "            self.flat_size + num_classes, 128, num_layers=2,\n",
    "            batch_first=True, bidirectional=True, dropout=0.3\n",
    "        )\n",
    "        self.frame_fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Velocity prediction\n",
    "        self.velocity_fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_mels, time)\n",
    "        x = x.unsqueeze(1)  # Add channel dim\n",
    "        \n",
    "        # CNN\n",
    "        x = self.conv_stack(x)  # (batch, 128, n_mels//4, time)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        batch, channels, freq, time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).reshape(batch, time, -1)\n",
    "        \n",
    "        # Onset detection\n",
    "        onset_features, _ = self.onset_lstm(x)\n",
    "        onset_pred = torch.sigmoid(self.onset_fc(onset_features))\n",
    "        \n",
    "        # Frame detection (with onset info)\n",
    "        frame_input = torch.cat([x, onset_pred], dim=-1)\n",
    "        frame_features, _ = self.frame_lstm(frame_input)\n",
    "        frame_pred = torch.sigmoid(self.frame_fc(frame_features))\n",
    "        \n",
    "        # Velocity\n",
    "        velocity_pred = torch.sigmoid(self.velocity_fc(onset_features))\n",
    "        \n",
    "        # Permute back: (batch, classes, time)\n",
    "        onset_pred = onset_pred.permute(0, 2, 1)\n",
    "        frame_pred = frame_pred.permute(0, 2, 1)\n",
    "        velocity_pred = velocity_pred.permute(0, 2, 1)\n",
    "        \n",
    "        return onset_pred, frame_pred, velocity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create datasets and dataloaders\ntrain_dataset = DrumDataset(DATA_DIR, split='train')\nval_dataset = DrumDataset(DATA_DIR, split='val')\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=True, \n    num_workers=4,\n    pin_memory=True\n)\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=False,\n    num_workers=4\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "model = DrumTranscriptionModel(\n",
    "    n_mels=CONFIG['n_mels'],\n",
    "    num_classes=CONFIG['num_classes']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "# Loss functions\n",
    "onset_criterion = nn.BCELoss()\n",
    "frame_criterion = nn.BCELoss()\n",
    "velocity_criterion = nn.MSELoss()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        mel_spec = batch['mel_spec'].to(device)\n",
    "        onsets_target = batch['onsets'].to(device)\n",
    "        frames_target = batch['frames'].to(device)\n",
    "        velocities_target = batch['velocities'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        onset_pred, frame_pred, velocity_pred = model(mel_spec)\n",
    "        \n",
    "        # Match dimensions\n",
    "        min_len = min(onset_pred.shape[-1], onsets_target.shape[-1])\n",
    "        onset_pred = onset_pred[..., :min_len]\n",
    "        frame_pred = frame_pred[..., :min_len]\n",
    "        velocity_pred = velocity_pred[..., :min_len]\n",
    "        onsets_target = onsets_target[..., :min_len]\n",
    "        frames_target = frames_target[..., :min_len]\n",
    "        velocities_target = velocities_target[..., :min_len]\n",
    "        \n",
    "        # Compute losses\n",
    "        onset_loss = onset_criterion(onset_pred, onsets_target)\n",
    "        frame_loss = frame_criterion(frame_pred, frames_target)\n",
    "        \n",
    "        # Velocity loss only where there are onsets\n",
    "        onset_mask = onsets_target > 0.5\n",
    "        if onset_mask.any():\n",
    "            vel_loss = velocity_criterion(\n",
    "                velocity_pred[onset_mask], \n",
    "                velocities_target[onset_mask]\n",
    "            )\n",
    "        else:\n",
    "            vel_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        loss = onset_loss + frame_loss + 0.5 * vel_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Validation'):\n",
    "            mel_spec = batch['mel_spec'].to(device)\n",
    "            onsets_target = batch['onsets'].to(device)\n",
    "            frames_target = batch['frames'].to(device)\n",
    "            \n",
    "            onset_pred, frame_pred, _ = model(mel_spec)\n",
    "            \n",
    "            min_len = min(onset_pred.shape[-1], onsets_target.shape[-1])\n",
    "            onset_pred = onset_pred[..., :min_len]\n",
    "            frame_pred = frame_pred[..., :min_len]\n",
    "            onsets_target = onsets_target[..., :min_len]\n",
    "            frames_target = frames_target[..., :min_len]\n",
    "            \n",
    "            onset_loss = onset_criterion(onset_pred, onsets_target)\n",
    "            frame_loss = frame_criterion(frame_pred, frames_target)\n",
    "            \n",
    "            total_loss += (onset_loss + frame_loss).item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nSAVE_DIR = Path('/content/drive/MyDrive/drum_model_results')\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\nbest_val_loss = float('inf')\nhistory = {'train_loss': [], 'val_loss': []}\n\nprint(\"Starting training...\")\nprint(f\"Saving checkpoints to: {SAVE_DIR}\")\n\nfor epoch in range(CONFIG['num_epochs']):\n    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n    \n    train_loss = train_epoch(model, train_loader, optimizer, device)\n    val_loss = validate(model, val_loader, device)\n    \n    scheduler.step(val_loss)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n            'config': CONFIG,\n            'class_names': CLASS_NAMES\n        }, SAVE_DIR / 'best_drum_model.pt')\n        print(f\"  Saved best model (val_loss: {val_loss:.4f})\")\n    \n    # Save checkpoint every 5 epochs (Colab can disconnect)\n    if (epoch + 1) % 5 == 0:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, SAVE_DIR / f'drum_model_epoch_{epoch+1}.pt')\n        print(f\"  Saved checkpoint at epoch {epoch+1}\")\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Drum Transcription Training')\n",
    "plt.savefig(SAVE_DIR / 'training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved models\n",
    "print(\"Saved models:\")\n",
    "!ls -la {SAVE_DIR}/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "Your trained drum transcription model is saved to `Google Drive/drum_model_results/`\n",
    "\n",
    "**Files:**\n",
    "- `best_drum_model.pt` - Best model checkpoint\n",
    "- `drum_model_epoch_XX.pt` - Periodic checkpoints\n",
    "- `training_history.png` - Loss curves\n",
    "\n",
    "**Next:** Copy the model to your StemScribe backend and update `drum_transcriber_v2.py` to use it."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}