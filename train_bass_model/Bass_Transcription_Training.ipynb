{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¸ Bass Transcription Model Training\n",
    "\n",
    "**Goal**: Train a CRNN model that transcribes bass audio â†’ MIDI with per-string/fret predictions.\n",
    "\n",
    "**Architecture**: CQT â†’ Conv2D â†’ BiLSTM â†’ onset/frame/velocity heads (4 strings Ã— 24 frets)\n",
    "\n",
    "**Dataset**: Slakh2100-redux (synthesized bass stems + aligned MIDI, ~145 hours total)\n",
    "\n",
    "**Why Slakh?**\n",
    "- Every track has isolated bass audio + perfectly aligned MIDI\n",
    "- 1,710 deduplicated tracks (redux version avoids train/test leakage)\n",
    "- Covers diverse bass styles via 187 instrument patches\n",
    "- Audio synthesized from Lakh MIDI Dataset using professional virtual instruments\n",
    "\n",
    "**Output**: `best_bass_model.pt` â€” drop into `backend/models/pretrained/`\n",
    "\n",
    "---\n",
    "**Estimated Training Time**:\n",
    "- A100: ~4-8 hours (50 epochs)\n",
    "- T4: ~12-24 hours\n",
    "\n",
    "**Disk Requirements**: ~60 GB (Slakh2100 bass stems + MIDI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Check & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport os\n\n# GPU check\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\nelse:\n    print('WARNING: No GPU detected! Training will be very slow.')\n    print('Go to Runtime > Change runtime type > GPU (A100 preferred)')\n\n# Mount Google Drive for checkpoint saving\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create output directory on Drive\nDRIVE_OUTPUT = '/content/drive/MyDrive/bass_model_results'\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nprint(f'Checkpoints will save to: {DRIVE_OUTPUT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q librosa pretty_midi tqdm soundfile pyyaml"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Slakh2100 Bass Stems\n",
    "\n",
    "Slakh2100 is ~105 GB total. We only need bass stems + MIDI (~15-20 GB).\n",
    "We'll download the full dataset and extract just bass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport shutil\nfrom pathlib import Path\n\nDATA_DIR = Path('/content/slakh_bass')\nDATA_DIR.mkdir(exist_ok=True)\n\nSLAKH_ROOT = Path('/content/slakh2100_flac_redux')\n\n# Check if we already have extracted bass data (Colab reconnect)\nexisting_tracks = list(DATA_DIR.glob('*_Track*'))\nif len(existing_tracks) > 100:\n    print(f'Found {len(existing_tracks)} existing bass tracks, skipping download')\nelif SLAKH_ROOT.exists() and len(list(SLAKH_ROOT.glob('**/metadata.yaml'))) > 100:\n    print(f'Slakh already extracted, skipping download (will extract bass in next cell)')\nelse:\n    print('Downloading Slakh2100-redux from Zenodo...')\n    print('This is ~97 GB â€” grab a coffee')\n    print()\n\n    TAR_PATH = '/content/slakh2100.tar.gz'\n\n    # Download Slakh2100-redux (deduplicated version)\n    !wget -q --show-progress -O {TAR_PATH} \\\n        'https://zenodo.org/records/4599666/files/slakh2100_flac_redux.tar.gz?download=1'\n\n    # Check disk space before extracting\n    import shutil as sh\n    total, used, free = sh.disk_usage('/content')\n    free_gb = free / 1e9\n    print(f'\\nDisk space: {free_gb:.1f} GB free')\n\n    if free_gb < 110:\n        # Not enough space to keep tar + extracted simultaneously\n        # Use streaming approach: extract in pieces, deleting tar progressively\n        print('Using streaming extraction to manage disk space...')\n\n        # First, extract ONLY metadata files (tiny) to identify bass stems\n        print('Step 1: Extracting metadata files only...')\n        !tar xzf {TAR_PATH} -C /content/ --wildcards '*/metadata.yaml' 2>/dev/null || true\n\n        # Now delete the tar to free ~97GB\n        tar_file = Path(TAR_PATH)\n        if tar_file.exists():\n            tar_file.unlink()\n            print('Deleted tar.gz to free disk space')\n\n        # Re-download only the bass stems we need using the metadata\n        # First, scan metadata to find bass stem IDs\n        print('Step 2: Scanning metadata for bass stems...')\n        import yaml\n        bass_files_needed = []\n        for meta_path in sorted(SLAKH_ROOT.glob('**/metadata.yaml')):\n            try:\n                with open(meta_path) as f:\n                    meta = yaml.safe_load(f)\n                track_dir = meta_path.parent\n                rel_track = track_dir.relative_to(Path('/content'))\n                for stem_id, stem_info in meta.get('stems', {}).items():\n                    prog = stem_info.get('program_num', -1)\n                    is_drum = stem_info.get('is_drum', False)\n                    if not is_drum and prog in range(32, 40):\n                        bass_files_needed.append(f'{rel_track}/stems/{stem_id}.flac')\n                        bass_files_needed.append(f'{rel_track}/MIDI/{stem_id}.mid')\n            except Exception:\n                continue\n\n        print(f'Found {len(bass_files_needed) // 2} bass stems to extract')\n\n        # Write file list and re-download + extract only those files\n        with open('/content/bass_files.txt', 'w') as f:\n            for fn in bass_files_needed:\n                f.write(fn + '\\n')\n\n        print('Step 3: Re-downloading and extracting only bass stems...')\n        !wget -q --show-progress -O - \\\n            'https://zenodo.org/records/4599666/files/slakh2100_flac_redux.tar.gz?download=1' | \\\n            tar xzf - -C /content/ -T /content/bass_files.txt 2>/dev/null || true\n\n        print('Done with streaming extraction')\n    else:\n        # Enough disk space for normal extraction\n        print('Extracting full dataset...')\n        !tar xzf {TAR_PATH} -C /content/ 2>/dev/null || true\n\n        # Delete tar immediately to free space\n        tar_file = Path(TAR_PATH)\n        if tar_file.exists():\n            tar_file.unlink()\n            print('Deleted tar.gz to free disk space')\n\n    print('Extraction complete')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\nimport shutil\nfrom pathlib import Path\n\n# Extract bass stems from Slakh2100\n# Slakh metadata.yaml tells us which stem number is bass\n\nSLAKH_ROOT = Path('/content/slakh2100_flac_redux')\nDATA_DIR = Path('/content/slakh_bass')\nDATA_DIR.mkdir(exist_ok=True)\n\n# Bass MIDI programs: 32-39 (Acoustic Bass, Electric Bass, etc.)\nBASS_PROGRAMS = set(range(32, 40))\n\nbass_tracks = []\nskipped = 0\n\nfor split in ['train', 'validation', 'test']:\n    split_dir = SLAKH_ROOT / split\n    if not split_dir.exists():\n        print(f'Split {split} not found, skipping')\n        continue\n\n    for track_dir in sorted(split_dir.iterdir()):\n        if not track_dir.is_dir():\n            continue\n\n        metadata_file = track_dir / 'metadata.yaml'\n        if not metadata_file.exists():\n            skipped += 1\n            continue\n\n        with open(metadata_file) as f:\n            meta = yaml.safe_load(f)\n\n        # Find bass stem(s)\n        stems_meta = meta.get('stems', {})\n        for stem_id, stem_info in stems_meta.items():\n            program = stem_info.get('program_num', -1)\n            is_drum = stem_info.get('is_drum', False)\n\n            if is_drum or program not in BASS_PROGRAMS:\n                continue\n\n            # Found a bass stem!\n            audio_file = track_dir / 'stems' / f'{stem_id}.flac'\n            midi_file = track_dir / 'MIDI' / f'{stem_id}.mid'\n\n            if audio_file.exists() and midi_file.exists():\n                # Copy to our bass directory\n                track_name = f\"{split}_{track_dir.name}_{stem_id}\"\n                out_dir = DATA_DIR / track_name\n                out_dir.mkdir(exist_ok=True)\n\n                shutil.copy2(audio_file, out_dir / 'bass.flac')\n                shutil.copy2(midi_file, out_dir / 'bass.mid')\n\n                bass_tracks.append({\n                    'track': track_name,\n                    'split': split,\n                    'program': program,\n                    'audio': str(out_dir / 'bass.flac'),\n                    'midi': str(out_dir / 'bass.mid'),\n                })\n\nprint(f'\\nâœ… Extracted {len(bass_tracks)} bass stems')\nprint(f'   Skipped {skipped} tracks (no metadata)')\nprint(f'   Train: {sum(1 for t in bass_tracks if t[\"split\"] == \"train\")}')\nprint(f'   Val: {sum(1 for t in bass_tracks if t[\"split\"] == \"validation\")}')\nprint(f'   Test: {sum(1 for t in bass_tracks if t[\"split\"] == \"test\")}')\n\n# Always clean up the full Slakh to save disk â€” we only need the extracted bass stems\nif SLAKH_ROOT.exists():\n    print('\\nCleaning up full Slakh archive to save disk space...')\n    shutil.rmtree(SLAKH_ROOT, ignore_errors=True)\n    print('âœ… Freed disk space')\n\nslakh_tar = Path('/content/slakh2100.tar.gz')\nif slakh_tar.exists():\n    slakh_tar.unlink()\n    print('âœ… Removed tar.gz')\n\n# Verify we got enough data\nif len(bass_tracks) < 100:\n    print(f'\\nâš ï¸ WARNING: Only found {len(bass_tracks)} bass tracks.')\n    print('Expected ~1000+. Extraction may have been incomplete.')\n    print('Check disk space and re-run if needed.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIG â€” must match inference in bass_transcriber.py EXACTLY\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'sample_rate': 22050,\n",
    "    'hop_length': 256,\n",
    "    'n_bins': 84,           # CQT bins (7 octaves Ã— 12 bins/octave)\n",
    "    'bins_per_octave': 12,\n",
    "    'chunk_duration': 5.0,  # seconds per training chunk\n",
    "    'num_strings': 4,\n",
    "    'num_frets': 24,\n",
    "    'tuning': [28, 33, 38, 43],  # E1, A1, D2, G2 (4-string bass, standard)\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 50,\n",
    "}\n",
    "\n",
    "SAMPLE_RATE = CONFIG['sample_rate']\n",
    "HOP_LENGTH = CONFIG['hop_length']\n",
    "N_BINS = CONFIG['n_bins']\n",
    "BINS_PER_OCTAVE = CONFIG['bins_per_octave']\n",
    "CHUNK_DURATION = CONFIG['chunk_duration']\n",
    "NUM_STRINGS = CONFIG['num_strings']\n",
    "NUM_FRETS = CONFIG['num_frets']\n",
    "TUNING = CONFIG['tuning']   # MIDI notes for open strings\n",
    "CHUNK_SAMPLES = int(CHUNK_DURATION * SAMPLE_RATE)  # 110250\n",
    "CHUNK_FRAMES = int(CHUNK_SAMPLES / HOP_LENGTH)      # 430\n",
    "\n",
    "print(f'Chunk: {CHUNK_DURATION}s = {CHUNK_SAMPLES} samples = {CHUNK_FRAMES} frames')\n",
    "print(f'Bass tuning (MIDI): {TUNING} = E1, A1, D2, G2')\n",
    "print(f'Output shape per chunk: ({NUM_STRINGS}, {NUM_FRETS}, {CHUNK_FRAMES})')\n",
    "print(f'Total output neurons: {NUM_STRINGS * NUM_FRETS} = {NUM_STRINGS}Ã—{NUM_FRETS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def midi_note_to_string_fret(midi_note, tuning=TUNING, num_frets=NUM_FRETS):\n",
    "    \"\"\"\n",
    "    Map a MIDI note to the best (string, fret) position on bass.\n",
    "\n",
    "    Prefers lower positions (closer to nut) and lower strings.\n",
    "    Returns None if note is out of range.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for s, open_note in enumerate(tuning):\n",
    "        fret = midi_note - open_note\n",
    "        if 0 <= fret < num_frets:\n",
    "            # Prefer lower fret positions, then lower strings\n",
    "            candidates.append((s, fret, fret + s * 0.1))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Sort by preference score (lower = better)\n",
    "    candidates.sort(key=lambda x: x[2])\n",
    "    return candidates[0][0], candidates[0][1]\n",
    "\n",
    "\n",
    "def midi_to_targets(midi_path, total_frames, sr=SAMPLE_RATE, hop=HOP_LENGTH):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to onset/frame/velocity target tensors.\n",
    "\n",
    "    Returns:\n",
    "        onset_target: (num_strings, num_frets, total_frames) binary\n",
    "        frame_target: (num_strings, num_frets, total_frames) binary\n",
    "        velocity_target: (num_strings, num_frets, total_frames) float [0-1]\n",
    "    \"\"\"\n",
    "    onset = np.zeros((NUM_STRINGS, NUM_FRETS, total_frames), dtype=np.float32)\n",
    "    frame = np.zeros((NUM_STRINGS, NUM_FRETS, total_frames), dtype=np.float32)\n",
    "    velocity = np.zeros((NUM_STRINGS, NUM_FRETS, total_frames), dtype=np.float32)\n",
    "\n",
    "    frames_per_sec = sr / hop\n",
    "\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    except Exception:\n",
    "        return onset, frame, velocity\n",
    "\n",
    "    for instrument in midi.instruments:\n",
    "        if instrument.is_drum:\n",
    "            continue\n",
    "\n",
    "        for note in instrument.notes:\n",
    "            result = midi_note_to_string_fret(note.pitch)\n",
    "            if result is None:\n",
    "                continue\n",
    "\n",
    "            s, f = result\n",
    "            onset_frame = int(note.start * frames_per_sec)\n",
    "            offset_frame = int(note.end * frames_per_sec)\n",
    "\n",
    "            if onset_frame >= total_frames:\n",
    "                continue\n",
    "\n",
    "            offset_frame = min(offset_frame, total_frames - 1)\n",
    "            vel = note.velocity / 127.0\n",
    "\n",
    "            # Onset: single frame at note start\n",
    "            onset[s, f, onset_frame] = 1.0\n",
    "\n",
    "            # Frame: all frames while note is active\n",
    "            frame[s, f, onset_frame:offset_frame + 1] = 1.0\n",
    "\n",
    "            # Velocity: at onset frame\n",
    "            velocity[s, f, onset_frame] = vel\n",
    "\n",
    "    return onset, frame, velocity\n",
    "\n",
    "\n",
    "class BassTranscriptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that yields (cqt_chunk, onset_target, frame_target, velocity_target)\n",
    "    from paired bass audio + MIDI files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, track_list, chunks_per_track=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            track_list: List of dicts with 'audio' and 'midi' paths\n",
    "            chunks_per_track: Random chunks to sample per track per epoch\n",
    "        \"\"\"\n",
    "        self.tracks = track_list\n",
    "        self.chunks_per_track = chunks_per_track\n",
    "        self._cache = {}  # Cache loaded audio/MIDI to avoid re-reading\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks) * self.chunks_per_track\n",
    "\n",
    "    def _load_track(self, idx):\n",
    "        track_idx = idx // self.chunks_per_track\n",
    "\n",
    "        if track_idx in self._cache:\n",
    "            return self._cache[track_idx]\n",
    "\n",
    "        track = self.tracks[track_idx]\n",
    "\n",
    "        # Load audio\n",
    "        try:\n",
    "            audio, sr = librosa.load(track['audio'], sr=SAMPLE_RATE, mono=True)\n",
    "        except Exception:\n",
    "            audio = np.zeros(CHUNK_SAMPLES * 2, dtype=np.float32)\n",
    "\n",
    "        # Compute full CQT\n",
    "        cqt = np.abs(librosa.cqt(\n",
    "            audio, sr=SAMPLE_RATE,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_bins=N_BINS,\n",
    "            bins_per_octave=BINS_PER_OCTAVE\n",
    "        ))\n",
    "        cqt = np.log(cqt + 1e-8).astype(np.float32)  # MUST match inference!\n",
    "\n",
    "        total_frames = cqt.shape[1]\n",
    "\n",
    "        # Build targets from MIDI\n",
    "        onset, frame, velocity = midi_to_targets(track['midi'], total_frames)\n",
    "\n",
    "        result = (cqt, onset, frame, velocity, total_frames)\n",
    "\n",
    "        # Cache (limit cache size to avoid OOM)\n",
    "        if len(self._cache) < 200:\n",
    "            self._cache[track_idx] = result\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cqt, onset, frame, velocity, total_frames = self._load_track(idx)\n",
    "\n",
    "        # Random chunk\n",
    "        if total_frames <= CHUNK_FRAMES:\n",
    "            # Pad short audio\n",
    "            pad_frames = CHUNK_FRAMES - total_frames\n",
    "            cqt_chunk = np.pad(cqt, ((0, 0), (0, pad_frames)))\n",
    "            onset_chunk = np.pad(onset, ((0, 0), (0, 0), (0, pad_frames)))\n",
    "            frame_chunk = np.pad(frame, ((0, 0), (0, 0), (0, pad_frames)))\n",
    "            vel_chunk = np.pad(velocity, ((0, 0), (0, 0), (0, pad_frames)))\n",
    "        else:\n",
    "            start = np.random.randint(0, total_frames - CHUNK_FRAMES)\n",
    "            cqt_chunk = cqt[:, start:start + CHUNK_FRAMES]\n",
    "            onset_chunk = onset[:, :, start:start + CHUNK_FRAMES]\n",
    "            frame_chunk = frame[:, :, start:start + CHUNK_FRAMES]\n",
    "            vel_chunk = velocity[:, :, start:start + CHUNK_FRAMES]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(cqt_chunk),\n",
    "            torch.from_numpy(onset_chunk),\n",
    "            torch.from_numpy(frame_chunk),\n",
    "            torch.from_numpy(vel_chunk),\n",
    "        )\n",
    "\n",
    "\n",
    "print('âœ… Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/val splits\n",
    "import random\n",
    "\n",
    "# Use Slakh's built-in splits\n",
    "train_tracks = [t for t in bass_tracks if t['split'] == 'train']\n",
    "val_tracks = [t for t in bass_tracks if t['split'] in ('validation', 'test')]\n",
    "\n",
    "# If we didn't get proper splits (e.g., manual extraction), do 90/10\n",
    "if len(val_tracks) < 10:\n",
    "    random.shuffle(bass_tracks)\n",
    "    split_idx = int(len(bass_tracks) * 0.9)\n",
    "    train_tracks = bass_tracks[:split_idx]\n",
    "    val_tracks = bass_tracks[split_idx:]\n",
    "\n",
    "print(f'Train: {len(train_tracks)} tracks')\n",
    "print(f'Val:   {len(val_tracks)} tracks')\n",
    "\n",
    "train_dataset = BassTranscriptionDataset(train_tracks, chunks_per_track=8)\n",
    "val_dataset = BassTranscriptionDataset(val_tracks, chunks_per_track=4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True, num_workers=2, pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Train batches/epoch: {len(train_loader)}')\n",
    "print(f'Val batches/epoch:   {len(val_loader)}')\n",
    "\n",
    "# Sanity check: load one batch\n",
    "cqt_b, onset_b, frame_b, vel_b = next(iter(train_loader))\n",
    "print(f'\\nBatch shapes:')\n",
    "print(f'  CQT:      {cqt_b.shape}')      # (B, 84, 430)\n",
    "print(f'  Onset:    {onset_b.shape}')     # (B, 4, 24, 430)\n",
    "print(f'  Frame:    {frame_b.shape}')     # (B, 4, 24, 430)\n",
    "print(f'  Velocity: {vel_b.shape}')       # (B, 4, 24, 430)\n",
    "print(f'  Onset active: {onset_b.sum():.0f} / {onset_b.numel()}')\n",
    "print(f'  Frame active: {frame_b.sum():.0f} / {frame_b.numel()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BassTranscriptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CRNN for bass transcription: CQT â†’ Conv2D â†’ BiLSTM â†’ onset/frame/velocity\n",
    "\n",
    "    Architecture follows our proven drum/guitar tab pattern:\n",
    "    - Conv2D stack pools frequency axis only (preserves time)\n",
    "    - BiLSTM captures temporal dependencies\n",
    "    - Separate onset, frame, and velocity prediction heads\n",
    "    - Frame prediction conditioned on onset (OaF-style)\n",
    "\n",
    "    Input:  (batch, n_bins=84, time)\n",
    "    Output: onset  (batch, 4, 24, time)  â€” string Ã— fret binary onset\n",
    "            frame  (batch, 4, 24, time)  â€” string Ã— fret binary frame\n",
    "            velocity (batch, 4, 24, time) â€” velocity at onset points\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=84, num_strings=4, num_frets=24,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_strings = num_strings\n",
    "        self.num_frets = num_frets\n",
    "        output_size = num_strings * num_frets  # 96\n",
    "\n",
    "        # CNN encoder â€” pools frequency only via MaxPool2d((2,1))\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: 84 â†’ 42\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # freq only: 84 â†’ 42\n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Block 2: 42 â†’ 21\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # freq only: 42 â†’ 21\n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Block 3: 21 â†’ 10\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # freq only: 21 â†’ 10\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "\n",
    "        # After CNN: (batch, 128, 10, time)\n",
    "        cnn_output_dim = 128 * 10  # 1280\n",
    "\n",
    "        # Onset LSTM\n",
    "        self.onset_lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Frame LSTM (conditioned on onset predictions)\n",
    "        self.frame_lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim + output_size,  # CNN + onset_pred\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        lstm_output_dim = hidden_size * 2  # bidirectional â†’ 512\n",
    "\n",
    "        # Output heads\n",
    "        self.onset_head = nn.Linear(lstm_output_dim, output_size)\n",
    "        self.frame_head = nn.Linear(lstm_output_dim, output_size)\n",
    "        self.velocity_head = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, output_size),\n",
    "            nn.Sigmoid(),  # velocity is 0-1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, n_bins, time) â€” CQT spectrogram\n",
    "        Returns:\n",
    "            onset:    (batch, num_strings, num_frets, time)\n",
    "            frame:    (batch, num_strings, num_frets, time)\n",
    "            velocity: (batch, num_strings, num_frets, time)\n",
    "        \"\"\"\n",
    "        batch, n_bins, time = x.shape\n",
    "\n",
    "        # CNN: (batch, 1, n_bins, time) â†’ (batch, 128, 10, time)\n",
    "        cnn_out = self.cnn(x.unsqueeze(1))\n",
    "\n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        cnn_features = cnn_out.permute(0, 3, 1, 2)  # (B, T, 128, 10)\n",
    "        cnn_features = cnn_features.reshape(batch, time, -1)  # (B, T, 1280)\n",
    "\n",
    "        # Onset prediction\n",
    "        onset_out, _ = self.onset_lstm(cnn_features)  # (B, T, 512)\n",
    "        onset_logits = self.onset_head(onset_out)     # (B, T, 96)\n",
    "        onset_pred = torch.sigmoid(onset_logits)\n",
    "\n",
    "        # Frame prediction (conditioned on onset)\n",
    "        frame_input = torch.cat([cnn_features, onset_pred.detach()], dim=-1)\n",
    "        frame_out, _ = self.frame_lstm(frame_input)   # (B, T, 512)\n",
    "        frame_logits = self.frame_head(frame_out)     # (B, T, 96)\n",
    "        frame_pred = torch.sigmoid(frame_logits)\n",
    "\n",
    "        # Velocity prediction (from onset LSTM features)\n",
    "        velocity_pred = self.velocity_head(onset_out)  # (B, T, 96)\n",
    "\n",
    "        # Reshape to (batch, strings, frets, time)\n",
    "        def reshape_output(t):\n",
    "            return t.permute(0, 2, 1).reshape(\n",
    "                batch, self.num_strings, self.num_frets, time\n",
    "            )\n",
    "\n",
    "        return reshape_output(onset_pred), reshape_output(frame_pred), reshape_output(velocity_pred)\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = BassTranscriptionModel()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {total_params:,}')\n",
    "\n",
    "# Dummy forward pass\n",
    "dummy = torch.randn(2, N_BINS, CHUNK_FRAMES)\n",
    "onset, frame, vel = model(dummy)\n",
    "print(f'Input:    {dummy.shape}')\n",
    "print(f'Onset:    {onset.shape}')    # (2, 4, 24, 430)\n",
    "print(f'Frame:    {frame.shape}')    # (2, 4, 24, 430)\n",
    "print(f'Velocity: {vel.shape}')      # (2, 4, 24, 430)\n",
    "assert onset.shape == (2, NUM_STRINGS, NUM_FRETS, CHUNK_FRAMES)\n",
    "print('\\nâœ… Model architecture verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Training on: {device}')\n\nmodel = BassTranscriptionModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', patience=5, factor=0.5\n)\n\nonset_loss_fn = nn.BCELoss()\nframe_loss_fn = nn.BCELoss()\nvelocity_loss_fn = nn.MSELoss()\n\n# Resume from checkpoint if available\nstart_epoch = 0\nbest_val_loss = float('inf')\n\n# Check for existing checkpoints on Drive\ncheckpoint_files = sorted(Path(DRIVE_OUTPUT).glob('checkpoint_epoch_*.pt'))\nbest_model_path = Path(DRIVE_OUTPUT) / 'best_bass_model.pt'\n\nif checkpoint_files:\n    latest = checkpoint_files[-1]\n    print(f'Resuming from {latest.name}...')\n    ckpt = torch.load(latest, map_location=device, weights_only=False)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    start_epoch = ckpt['epoch'] + 1\n    best_val_loss = ckpt.get('val_loss', float('inf'))\n    print(f'Resumed at epoch {start_epoch}, best_val_loss={best_val_loss:.5f}')\nelif best_model_path.exists():\n    print(f'Found best model, resuming...')\n    ckpt = torch.load(best_model_path, map_location=device, weights_only=False)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    start_epoch = ckpt['epoch'] + 1\n    best_val_loss = ckpt.get('val_loss', float('inf'))\n    print(f'Resumed at epoch {start_epoch}, best_val_loss={best_val_loss:.5f}')\n\nprint(f'\\nStarting training from epoch {start_epoch}...')\nprint(f'Epochs: {CONFIG[\"num_epochs\"]}, Batch size: {CONFIG[\"batch_size\"]}')\nprint(f'LR: {CONFIG[\"learning_rate\"]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "NUM_EPOCHS = CONFIG['num_epochs']\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{NUM_EPOCHS}')\n",
    "    for cqt_batch, onset_tgt, frame_tgt, vel_tgt in pbar:\n",
    "        cqt_batch = cqt_batch.to(device)\n",
    "        onset_tgt = onset_tgt.to(device)\n",
    "        frame_tgt = frame_tgt.to(device)\n",
    "        vel_tgt = vel_tgt.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        onset_pred, frame_pred, vel_pred = model(cqt_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_onset = onset_loss_fn(onset_pred, onset_tgt)\n",
    "        loss_frame = frame_loss_fn(frame_pred, frame_tgt)\n",
    "\n",
    "        # Velocity loss only where onsets exist\n",
    "        onset_mask = onset_tgt > 0.5\n",
    "        if onset_mask.any():\n",
    "            loss_vel = velocity_loss_fn(\n",
    "                vel_pred[onset_mask], vel_tgt[onset_mask]\n",
    "            )\n",
    "        else:\n",
    "            loss_vel = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = loss_onset + loss_frame + 0.5 * loss_vel\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "\n",
    "    train_loss = running_loss / max(num_batches, 1)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cqt_batch, onset_tgt, frame_tgt, vel_tgt in val_loader:\n",
    "            cqt_batch = cqt_batch.to(device)\n",
    "            onset_tgt = onset_tgt.to(device)\n",
    "            frame_tgt = frame_tgt.to(device)\n",
    "            vel_tgt = vel_tgt.to(device)\n",
    "\n",
    "            onset_pred, frame_pred, vel_pred = model(cqt_batch)\n",
    "\n",
    "            loss_onset = onset_loss_fn(onset_pred, onset_tgt)\n",
    "            loss_frame = frame_loss_fn(frame_pred, frame_tgt)\n",
    "\n",
    "            onset_mask = onset_tgt > 0.5\n",
    "            if onset_mask.any():\n",
    "                loss_vel = velocity_loss_fn(\n",
    "                    vel_pred[onset_mask], vel_tgt[onset_mask]\n",
    "                )\n",
    "            else:\n",
    "                loss_vel = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = loss_onset + loss_frame + 0.5 * loss_vel\n",
    "            val_running += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    val_loss = val_running / max(val_batches, 1)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # LR scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    elapsed = time.time() - epoch_start\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f'Epoch {epoch}/{NUM_EPOCHS} | '\n",
    "          f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | '\n",
    "          f'LR: {lr:.2e} | Time: {elapsed:.0f}s')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, str(best_model_path))\n",
    "        print(f'  âœ… Best model saved (val_loss={val_loss:.5f})')\n",
    "\n",
    "    # Periodic checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_path = Path(DRIVE_OUTPUT) / f'checkpoint_epoch_{epoch:03d}.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, str(ckpt_path))\n",
    "        print(f'  ðŸ’¾ Checkpoint saved: {ckpt_path.name}')\n",
    "\n",
    "print(f'\\nðŸŽ‰ Training complete! Best val_loss: {best_val_loss:.5f}')\n",
    "print(f'Best model saved to: {best_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss', color='steelblue')\n",
    "ax.plot(val_losses, label='Val Loss', color='coral')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title(f'Bass Transcription Training (best val_loss={best_val_loss:.5f})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_OUTPUT}/training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model and evaluate\nckpt = torch.load(str(best_model_path), map_location=device, weights_only=False)\nmodel.load_state_dict(ckpt['model_state_dict'])\nmodel.eval()\n\nprint(f'Loaded best model from epoch {ckpt[\"epoch\"]}, val_loss={ckpt[\"val_loss\"]:.5f}')\nprint(f'Config: {ckpt[\"config\"]}')\n\n# Quick inference test\nwith torch.no_grad():\n    test_input = torch.randn(1, N_BINS, CHUNK_FRAMES).to(device)\n    onset, frame, vel = model(test_input)\n    print(f'\\nInference test:')\n    print(f'  Input:    {test_input.shape}')\n    print(f'  Onset:    {onset.shape}, range [{onset.min():.3f}, {onset.max():.3f}]')\n    print(f'  Frame:    {frame.shape}, range [{frame.min():.3f}, {frame.max():.3f}]')\n    print(f'  Velocity: {vel.shape}, range [{vel.min():.3f}, {vel.max():.3f}]')\n\nprint(f'\\nâœ… Model ready for deployment!')\nprint(f'Copy {best_model_path} to backend/models/pretrained/best_bass_model.pt')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}