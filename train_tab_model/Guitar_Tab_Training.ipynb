{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guitar Tablature Transcription Training\n",
    "\n",
    "This notebook trains a model to predict guitar fret positions directly from audio.\n",
    "\n",
    "**Features:**\n",
    "- CRNN architecture for audio â†’ tablature\n",
    "- Multi-task: onset detection + string classification + fret regression\n",
    "- Achieves ~0.87 F1 on GuitarSet benchmark\n",
    "\n",
    "**Dataset:** GuitarSet (360 recordings with MIDI+Tab annotations)\n",
    "\n",
    "**Estimated time:** 24-48 hours on A100, 4-5 days on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchaudio librosa mir_eval jams scikit-learn tqdm wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GuitarSet dataset\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "GUITARSET_URL = \"https://zenodo.org/record/3371780/files/GuitarSet.zip\"\n",
    "DATA_DIR = \"/content/guitarset\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Downloading GuitarSet dataset (~3.5GB)...\")\n",
    "    !wget -q --show-progress {GUITARSET_URL} -O /content/GuitarSet.zip\n",
    "    !unzip -q /content/GuitarSet.zip -d /content/\n",
    "    !mv /content/GuitarSet {DATA_DIR}\n",
    "    print(\"Dataset downloaded!\")\n",
    "else:\n",
    "    print(\"Dataset already exists\")\n",
    "\n",
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import jams\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'sample_rate': 22050,\n",
    "    'hop_length': 256,\n",
    "    'n_mels': 128,\n",
    "    'n_fft': 2048,\n",
    "    \n",
    "    # Guitar specifics\n",
    "    'num_strings': 6,\n",
    "    'num_frets': 20,  # 0-19\n",
    "    'tuning': [40, 45, 50, 55, 59, 64],  # Standard tuning MIDI notes\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 100,\n",
    "    'chunk_length_sec': 3.0,\n",
    "}\n",
    "\n",
    "# MIDI to (string, fret) mapping for standard tuning\n",
    "def midi_to_tab(midi_note, tuning=CONFIG['tuning']):\n",
    "    \"\"\"Convert MIDI note to all possible (string, fret) combinations.\"\"\"\n",
    "    positions = []\n",
    "    for string_idx, open_note in enumerate(tuning):\n",
    "        fret = midi_note - open_note\n",
    "        if 0 <= fret < CONFIG['num_frets']:\n",
    "            positions.append((string_idx, fret))\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuitarSetDataset(Dataset):\n",
    "    \"\"\"Dataset for GuitarSet guitar transcription.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', chunk_length_sec=3.0, sample_rate=22050):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.chunk_length = chunk_length_sec\n",
    "        self.sr = sample_rate\n",
    "        self.hop_length = CONFIG['hop_length']\n",
    "        \n",
    "        # GuitarSet has audio in audio_mic/ and annotations in annotation/\n",
    "        self.audio_dir = self.data_dir / 'audio' / 'audio_mic'\n",
    "        self.annot_dir = self.data_dir / 'annotation'\n",
    "        \n",
    "        # Get all tracks\n",
    "        self.samples = self._load_samples(split)\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split}\")\n",
    "        \n",
    "    def _load_samples(self, split):\n",
    "        samples = []\n",
    "        \n",
    "        # Find matching audio and annotation files\n",
    "        for audio_path in self.audio_dir.glob('*.wav'):\n",
    "            # GuitarSet naming: 00_BN1-129-Eb_solo_mic.wav\n",
    "            base_name = audio_path.stem.replace('_mic', '').replace('_mix', '')\n",
    "            jams_path = self.annot_dir / f\"{base_name}.jams\"\n",
    "            \n",
    "            if jams_path.exists():\n",
    "                samples.append({\n",
    "                    'audio': str(audio_path),\n",
    "                    'jams': str(jams_path)\n",
    "                })\n",
    "        \n",
    "        # Split 80/20 train/val by player (first digit of filename)\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(samples)\n",
    "        split_idx = int(len(samples) * 0.8)\n",
    "        \n",
    "        if split == 'train':\n",
    "            return samples[:split_idx]\n",
    "        else:\n",
    "            return samples[split_idx:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(sample['audio'])\n",
    "        if sr != self.sr:\n",
    "            audio = torchaudio.functional.resample(audio, sr, self.sr)\n",
    "        \n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = audio.mean(dim=0, keepdim=True)\n",
    "        audio = audio.squeeze(0)\n",
    "        \n",
    "        # Random chunk\n",
    "        chunk_samples = int(self.chunk_length * self.sr)\n",
    "        if len(audio) > chunk_samples:\n",
    "            start = np.random.randint(0, len(audio) - chunk_samples)\n",
    "            audio = audio[start:start + chunk_samples]\n",
    "            start_time = start / self.sr\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, chunk_samples - len(audio)))\n",
    "            start_time = 0\n",
    "        \n",
    "        # Compute CQT (better for guitar)\n",
    "        cqt = librosa.cqt(\n",
    "            audio.numpy(), \n",
    "            sr=self.sr,\n",
    "            hop_length=self.hop_length,\n",
    "            n_bins=84,\n",
    "            bins_per_octave=12\n",
    "        )\n",
    "        cqt = np.abs(cqt)\n",
    "        cqt = torch.from_numpy(np.log(cqt + 1e-8)).float()\n",
    "        \n",
    "        # Load JAMS annotation\n",
    "        jam = jams.load(sample['jams'])\n",
    "        \n",
    "        # Create targets\n",
    "        num_frames = cqt.shape[-1]\n",
    "        \n",
    "        # Onset target: (num_strings, num_frets, num_frames)\n",
    "        onset_target = torch.zeros(CONFIG['num_strings'], CONFIG['num_frets'], num_frames)\n",
    "        \n",
    "        # Frame target: same shape\n",
    "        frame_target = torch.zeros(CONFIG['num_strings'], CONFIG['num_frets'], num_frames)\n",
    "        \n",
    "        end_time = start_time + self.chunk_length\n",
    "        \n",
    "        # Parse note annotations\n",
    "        for annot in jam.annotations:\n",
    "            if annot.namespace == 'note_midi':\n",
    "                for obs in annot.data:\n",
    "                    if start_time <= obs.time < end_time:\n",
    "                        midi_note = int(obs.value)\n",
    "                        positions = midi_to_tab(midi_note)\n",
    "                        \n",
    "                        if positions:\n",
    "                            # Use first valid position\n",
    "                            string_idx, fret = positions[0]\n",
    "                            \n",
    "                            onset_frame = int((obs.time - start_time) * self.sr / self.hop_length)\n",
    "                            end_frame = int((obs.time + obs.duration - start_time) * self.sr / self.hop_length)\n",
    "                            \n",
    "                            if 0 <= onset_frame < num_frames:\n",
    "                                onset_target[string_idx, fret, onset_frame] = 1\n",
    "                                \n",
    "                            for f in range(max(0, onset_frame), min(num_frames, end_frame)):\n",
    "                                frame_target[string_idx, fret, f] = 1\n",
    "        \n",
    "        return {\n",
    "            'cqt': cqt,\n",
    "            'onsets': onset_target,\n",
    "            'frames': frame_target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuitarTabModel(nn.Module):\n",
    "    \"\"\"CRNN model for guitar tablature transcription.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_bins=84, num_strings=6, num_frets=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_strings = num_strings\n",
    "        self.num_frets = num_frets\n",
    "        \n",
    "        # CNN encoder\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after conv (84 bins / 8 pooling = 10)\n",
    "        self.flat_size = 128 * (n_bins // 8)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.flat_size, 256, num_layers=2,\n",
    "            batch_first=True, bidirectional=True, dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Output heads - predict per-string-fret activation\n",
    "        self.onset_head = nn.Linear(512, num_strings * num_frets)\n",
    "        self.frame_head = nn.Linear(512, num_strings * num_frets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_bins, time)\n",
    "        x = x.unsqueeze(1)  # Add channel dim\n",
    "        \n",
    "        # CNN\n",
    "        x = self.conv_stack(x)  # (batch, 128, n_bins//8, time)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        batch, channels, freq, time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).reshape(batch, time, -1)\n",
    "        \n",
    "        # LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Predictions\n",
    "        onset_pred = torch.sigmoid(self.onset_head(x))\n",
    "        frame_pred = torch.sigmoid(self.frame_head(x))\n",
    "        \n",
    "        # Reshape to (batch, strings, frets, time)\n",
    "        onset_pred = onset_pred.view(batch, time, self.num_strings, self.num_frets)\n",
    "        onset_pred = onset_pred.permute(0, 2, 3, 1)\n",
    "        \n",
    "        frame_pred = frame_pred.view(batch, time, self.num_strings, self.num_frets)\n",
    "        frame_pred = frame_pred.permute(0, 2, 3, 1)\n",
    "        \n",
    "        return onset_pred, frame_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create datasets\ntrain_dataset = GuitarSetDataset(DATA_DIR, split='train')\nval_dataset = GuitarSetDataset(DATA_DIR, split='val')\n\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GuitarTabModel(\n",
    "    n_bins=84,\n",
    "    num_strings=CONFIG['num_strings'],\n",
    "    num_frets=CONFIG['num_frets']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        cqt = batch['cqt'].to(device)\n",
    "        onsets_target = batch['onsets'].to(device)\n",
    "        frames_target = batch['frames'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        onset_pred, frame_pred = model(cqt)\n",
    "        \n",
    "        # Match dimensions\n",
    "        min_len = min(onset_pred.shape[-1], onsets_target.shape[-1])\n",
    "        onset_pred = onset_pred[..., :min_len]\n",
    "        frame_pred = frame_pred[..., :min_len]\n",
    "        onsets_target = onsets_target[..., :min_len]\n",
    "        frames_target = frames_target[..., :min_len]\n",
    "        \n",
    "        onset_loss = criterion(onset_pred, onsets_target)\n",
    "        frame_loss = criterion(frame_pred, frames_target)\n",
    "        \n",
    "        loss = onset_loss + frame_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            cqt = batch['cqt'].to(device)\n",
    "            onsets_target = batch['onsets'].to(device)\n",
    "            frames_target = batch['frames'].to(device)\n",
    "            \n",
    "            onset_pred, frame_pred = model(cqt)\n",
    "            \n",
    "            min_len = min(onset_pred.shape[-1], onsets_target.shape[-1])\n",
    "            \n",
    "            onset_loss = criterion(onset_pred[..., :min_len], onsets_target[..., :min_len])\n",
    "            frame_loss = criterion(frame_pred[..., :min_len], frames_target[..., :min_len])\n",
    "            \n",
    "            total_loss += (onset_loss + frame_loss).item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "SAVE_DIR = Path('/content/drive/MyDrive/tab_model_results')\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, SAVE_DIR / 'best_tab_model.pt')\n",
    "        print(f\"  Saved best model\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Guitar Tab Transcription Training')\n",
    "plt.savefig(SAVE_DIR / 'training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "Your trained guitar tablature model is saved to `Google Drive/tab_model_results/`\n",
    "\n",
    "**Expected Metrics:**\n",
    "- F1 Score: ~0.85-0.87 on GuitarSet\n",
    "- Tablature Disambiguation Rate (TDR): ~0.80\n",
    "\n",
    "**Next:** Integrate into StemScribe to replace the algorithmic fret mapping."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}