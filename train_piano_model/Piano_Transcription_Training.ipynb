{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¹ Piano Transcription Model Training\n",
    "\n",
    "**Goal**: Train a CRNN model that transcribes piano audio â†’ MIDI with onset, frame, and velocity.\n",
    "\n",
    "**Architecture**: Mel â†’ Conv2D â†’ BiLSTM â†’ onset/frame/velocity heads (88 piano keys)\n",
    "\n",
    "**Dataset**: MAESTRO v3.0.0 (198 hours of Yamaha Disklavier piano with ~3ms MIDI alignment)\n",
    "\n",
    "**Why MAESTRO?**\n",
    "- Gold standard for piano AMT (Automatic Music Transcription)\n",
    "- ~3ms alignment between audio and MIDI from Disklavier hardware capture\n",
    "- 198 hours across 1,276 performances from International Piano-e-Competition\n",
    "- Full velocity, pedal (sustain, sostenuto, una corda) annotations\n",
    "- Used by Google Magenta, ByteDance, Sony AI for SOTA piano models\n",
    "\n",
    "**Output**: `best_piano_model.pt` â€” drop into `backend/models/pretrained/`\n",
    "\n",
    "---\n",
    "**Estimated Training Time**:\n",
    "- A100: ~6-12 hours (50 epochs)\n",
    "- T4: ~18-36 hours\n",
    "\n",
    "**Disk Requirements**: ~120 GB (MAESTRO WAV + MIDI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Check & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport os\n\n# GPU check\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\nelse:\n    print('WARNING: No GPU detected! Training will be very slow.')\n    print('Go to Runtime > Change runtime type > GPU (A100 preferred)')\n\n# Mount Google Drive for checkpoint saving\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create output directory on Drive\nDRIVE_OUTPUT = '/content/drive/MyDrive/piano_model_results'\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nprint(f'Checkpoints will save to: {DRIVE_OUTPUT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q librosa pretty_midi tqdm soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download MAESTRO v3.0.0\n",
    "\n",
    "~120 GB WAV dataset. MIDI-only is 58 MB but we need the audio too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nfrom pathlib import Path\nimport json\nimport shutil as sh\n\nMAESTRO_DIR = Path('/content/maestro-v3.0.0')\n\n# Check if already downloaded (reconnect-safe: need 1200+ WAV files for full dataset)\nif MAESTRO_DIR.exists() and len(list(MAESTRO_DIR.glob('**/*.wav'))) > 1200:\n    print(f'MAESTRO already downloaded: {len(list(MAESTRO_DIR.glob(\"**/*.wav\")))} WAV files')\nelse:\n    print('Downloading MAESTRO v3.0.0 from Google Storage...')\n    print('This is ~101 GB â€” will take 15-30 minutes on Colab')\n    print()\n\n    ZIP_PATH = '/content/maestro-v3.0.0.zip'\n\n    # Check available disk space\n    total, used, free = sh.disk_usage('/content')\n    free_gb = free / 1e9\n    print(f'Disk space: {free_gb:.1f} GB free')\n\n    if free_gb < 210:\n        # Not enough to hold zip + extracted simultaneously\n        # Use gsutil to download files directly (no zip needed)\n        print('Using gsutil to download directly (avoids zip disk overflow)...')\n        print('This downloads files individually â€” slower but disk-safe.')\n\n        !mkdir -p /content/maestro-v3.0.0\n        !gsutil -m cp -r 'gs://magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0/*' /content/maestro-v3.0.0/\n\n    else:\n        # Plenty of space â€” use the zip (faster)\n        !wget -q --show-progress -O {ZIP_PATH} \\\n            'https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.zip'\n\n        print('\\nExtracting...')\n        !cd /content && unzip -q maestro-v3.0.0.zip\n\n        # Delete zip immediately to free ~101 GB\n        zip_path = Path(ZIP_PATH)\n        if zip_path.exists():\n            zip_path.unlink()\n            print('Deleted zip to free disk space')\n\n# Verify\nwav_files = list(MAESTRO_DIR.glob('**/*.wav'))\nmidi_files = list(MAESTRO_DIR.glob('**/*.midi'))\nprint(f'\\nâœ… MAESTRO v3.0.0: {len(wav_files)} WAV files, {len(midi_files)} MIDI files')\n\nif len(wav_files) < 1200:\n    print(f'âš ï¸ WARNING: Expected ~1276 WAV files, got {len(wav_files)}.')\n    print('Dataset may be incomplete. Check disk space and re-run.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Parse the MAESTRO metadata CSV for train/val/test splits\n",
    "metadata_csv = MAESTRO_DIR / 'maestro-v3.0.0.csv'\n",
    "\n",
    "tracks = {'train': [], 'validation': [], 'test': []}\n",
    "\n",
    "with open(metadata_csv) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        split = row['split']\n",
    "        audio_path = MAESTRO_DIR / row['audio_filename']\n",
    "        midi_path = MAESTRO_DIR / row['midi_filename']\n",
    "\n",
    "        if audio_path.exists() and midi_path.exists():\n",
    "            tracks[split].append({\n",
    "                'audio': str(audio_path),\n",
    "                'midi': str(midi_path),\n",
    "                'duration': float(row['duration']),\n",
    "                'composer': row.get('canonical_composer', 'Unknown'),\n",
    "                'title': row.get('canonical_title', 'Unknown'),\n",
    "            })\n",
    "\n",
    "for split, track_list in tracks.items():\n",
    "    total_hours = sum(t['duration'] for t in track_list) / 3600\n",
    "    print(f'{split:12s}: {len(track_list):4d} tracks, {total_hours:.1f} hours')\n",
    "\n",
    "print(f'\\nTotal: {sum(len(v) for v in tracks.values())} tracks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIG â€” must match inference in piano_transcriber.py EXACTLY\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'sample_rate': 16000,    # 16kHz (standard for piano AMT)\n",
    "    'hop_length': 256,\n",
    "    'n_mels': 229,           # 229 mel bins (same as Onsets & Frames)\n",
    "    'n_fft': 2048,\n",
    "    'fmin': 30.0,            # A0 = 27.5 Hz, with some margin\n",
    "    'fmax': 8000.0,          # Covers piano range + harmonics\n",
    "    'chunk_duration': 5.0,   # seconds per training chunk\n",
    "    'num_keys': 88,          # A0 (21) to C8 (108) â€” full piano\n",
    "    'min_midi': 21,          # A0\n",
    "    'max_midi': 108,         # C8\n",
    "    'batch_size': 12,        # Slightly smaller â€” piano model is heavier\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 50,\n",
    "}\n",
    "\n",
    "SAMPLE_RATE = CONFIG['sample_rate']\n",
    "HOP_LENGTH = CONFIG['hop_length']\n",
    "N_MELS = CONFIG['n_mels']\n",
    "N_FFT = CONFIG['n_fft']\n",
    "NUM_KEYS = CONFIG['num_keys']\n",
    "MIN_MIDI = CONFIG['min_midi']\n",
    "MAX_MIDI = CONFIG['max_midi']\n",
    "CHUNK_DURATION = CONFIG['chunk_duration']\n",
    "CHUNK_SAMPLES = int(CHUNK_DURATION * SAMPLE_RATE)  # 80000\n",
    "CHUNK_FRAMES = int(CHUNK_SAMPLES / HOP_LENGTH)      # 312\n",
    "\n",
    "print(f'Chunk: {CHUNK_DURATION}s = {CHUNK_SAMPLES} samples = {CHUNK_FRAMES} frames')\n",
    "print(f'Piano range: MIDI {MIN_MIDI}-{MAX_MIDI} ({NUM_KEYS} keys)')\n",
    "print(f'Mel: {N_MELS} bins, {SAMPLE_RATE} Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def midi_to_piano_targets(midi_path, total_frames, sr=SAMPLE_RATE, hop=HOP_LENGTH):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to onset/frame/velocity targets for 88-key piano.\n",
    "\n",
    "    Returns:\n",
    "        onset:    (88, total_frames) binary\n",
    "        frame:    (88, total_frames) binary\n",
    "        velocity: (88, total_frames) float [0-1]\n",
    "    \"\"\"\n",
    "    onset = np.zeros((NUM_KEYS, total_frames), dtype=np.float32)\n",
    "    frame = np.zeros((NUM_KEYS, total_frames), dtype=np.float32)\n",
    "    velocity = np.zeros((NUM_KEYS, total_frames), dtype=np.float32)\n",
    "\n",
    "    frames_per_sec = sr / hop\n",
    "\n",
    "    try:\n",
    "        midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    except Exception:\n",
    "        return onset, frame, velocity\n",
    "\n",
    "    for instrument in midi.instruments:\n",
    "        if instrument.is_drum:\n",
    "            continue\n",
    "\n",
    "        for note in instrument.notes:\n",
    "            # Map MIDI note to piano key index (0-87)\n",
    "            key_idx = note.pitch - MIN_MIDI\n",
    "            if key_idx < 0 or key_idx >= NUM_KEYS:\n",
    "                continue\n",
    "\n",
    "            onset_frame = int(note.start * frames_per_sec)\n",
    "            offset_frame = int(note.end * frames_per_sec)\n",
    "\n",
    "            if onset_frame >= total_frames:\n",
    "                continue\n",
    "\n",
    "            offset_frame = min(offset_frame, total_frames - 1)\n",
    "            vel = note.velocity / 127.0\n",
    "\n",
    "            # Onset: single frame\n",
    "            onset[key_idx, onset_frame] = 1.0\n",
    "\n",
    "            # Frame: all frames while note sounds\n",
    "            frame[key_idx, onset_frame:offset_frame + 1] = 1.0\n",
    "\n",
    "            # Velocity: at onset\n",
    "            velocity[key_idx, onset_frame] = vel\n",
    "\n",
    "    return onset, frame, velocity\n",
    "\n",
    "\n",
    "class PianoTranscriptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset: yields (mel_chunk, onset_target, frame_target, velocity_target)\n",
    "    from MAESTRO audio + MIDI pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, track_list, chunks_per_track=6):\n",
    "        self.tracks = track_list\n",
    "        self.chunks_per_track = chunks_per_track\n",
    "        self._cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks) * self.chunks_per_track\n",
    "\n",
    "    def _load_track(self, idx):\n",
    "        track_idx = idx // self.chunks_per_track\n",
    "\n",
    "        if track_idx in self._cache:\n",
    "            return self._cache[track_idx]\n",
    "\n",
    "        track = self.tracks[track_idx]\n",
    "\n",
    "        # Load audio\n",
    "        try:\n",
    "            audio, sr = librosa.load(track['audio'], sr=SAMPLE_RATE, mono=True)\n",
    "        except Exception:\n",
    "            audio = np.zeros(CHUNK_SAMPLES * 2, dtype=np.float32)\n",
    "\n",
    "        # Compute mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=SAMPLE_RATE,\n",
    "            n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "            n_mels=N_MELS, fmin=CONFIG['fmin'], fmax=CONFIG['fmax']\n",
    "        )\n",
    "        mel = np.log(mel + 1e-8).astype(np.float32)  # MUST match inference!\n",
    "\n",
    "        total_frames = mel.shape[1]\n",
    "\n",
    "        # Build targets from MIDI\n",
    "        onset, frame, velocity = midi_to_piano_targets(track['midi'], total_frames)\n",
    "\n",
    "        result = (mel, onset, frame, velocity, total_frames)\n",
    "\n",
    "        # Cache (limit to avoid OOM â€” MAESTRO tracks are long)\n",
    "        if len(self._cache) < 50:\n",
    "            self._cache[track_idx] = result\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel, onset, frame, velocity, total_frames = self._load_track(idx)\n",
    "\n",
    "        # Random chunk\n",
    "        if total_frames <= CHUNK_FRAMES:\n",
    "            pad = CHUNK_FRAMES - total_frames\n",
    "            mel_chunk = np.pad(mel, ((0, 0), (0, pad)))\n",
    "            onset_chunk = np.pad(onset, ((0, 0), (0, pad)))\n",
    "            frame_chunk = np.pad(frame, ((0, 0), (0, pad)))\n",
    "            vel_chunk = np.pad(velocity, ((0, 0), (0, pad)))\n",
    "        else:\n",
    "            start = np.random.randint(0, total_frames - CHUNK_FRAMES)\n",
    "            mel_chunk = mel[:, start:start + CHUNK_FRAMES]\n",
    "            onset_chunk = onset[:, start:start + CHUNK_FRAMES]\n",
    "            frame_chunk = frame[:, start:start + CHUNK_FRAMES]\n",
    "            vel_chunk = velocity[:, start:start + CHUNK_FRAMES]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(mel_chunk),\n",
    "            torch.from_numpy(onset_chunk),\n",
    "            torch.from_numpy(frame_chunk),\n",
    "            torch.from_numpy(vel_chunk),\n",
    "        )\n",
    "\n",
    "\n",
    "print('âœ… Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data loaders from MAESTRO splits\n",
    "\n",
    "train_tracks = tracks['train']\n",
    "val_tracks = tracks['validation']\n",
    "\n",
    "print(f'Train: {len(train_tracks)} tracks')\n",
    "print(f'Val:   {len(val_tracks)} tracks')\n",
    "\n",
    "train_dataset = PianoTranscriptionDataset(train_tracks, chunks_per_track=6)\n",
    "val_dataset = PianoTranscriptionDataset(val_tracks, chunks_per_track=4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True, num_workers=2, pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'Train batches/epoch: {len(train_loader)}')\n",
    "print(f'Val batches/epoch:   {len(val_loader)}')\n",
    "\n",
    "# Sanity check\n",
    "mel_b, onset_b, frame_b, vel_b = next(iter(train_loader))\n",
    "print(f'\\nBatch shapes:')\n",
    "print(f'  Mel:      {mel_b.shape}')      # (B, 229, 312)\n",
    "print(f'  Onset:    {onset_b.shape}')     # (B, 88, 312)\n",
    "print(f'  Frame:    {frame_b.shape}')     # (B, 88, 312)\n",
    "print(f'  Velocity: {vel_b.shape}')       # (B, 88, 312)\n",
    "print(f'  Onset active: {onset_b.sum():.0f} / {onset_b.numel()}')\n",
    "print(f'  Frame active: {frame_b.sum():.0f} / {frame_b.numel()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Piano model uses a larger architecture than drums/bass since:\n",
    "- 88 output keys (vs 8 drums or 96 bass stringÃ—fret)\n",
    "- Highly polyphonic (10+ simultaneous notes)\n",
    "- Needs velocity sensitivity for expressive dynamics\n",
    "- 229 mel bins (vs 128 for drums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PianoTranscriptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CRNN for piano transcription: Mel â†’ Conv2D â†’ BiLSTM â†’ onset/frame/velocity\n",
    "\n",
    "    Inspired by Onsets and Frames (Hawthorne et al. 2018) with:\n",
    "    - Frame prediction conditioned on onset prediction\n",
    "    - Velocity regression head\n",
    "    - BatchNorm + Dropout for regularization\n",
    "\n",
    "    Input:  (batch, n_mels=229, time)\n",
    "    Output: onset    (batch, 88, time)  â€” per-key onset probability\n",
    "            frame    (batch, 88, time)  â€” per-key frame activation\n",
    "            velocity (batch, 88, time)  â€” velocity at onsets [0-1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_mels=229, num_keys=88,\n",
    "                 hidden_size=256, num_layers=2, dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_keys = num_keys\n",
    "\n",
    "        # CNN encoder â€” pools frequency only via MaxPool2d((2,1))\n",
    "        # 229 â†’ 114 â†’ 57 â†’ 28 â†’ 14\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: 229 â†’ 114\n",
    "            nn.Conv2d(1, 48, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # 229 â†’ 114\n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Block 2: 114 â†’ 57\n",
    "            nn.Conv2d(48, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # 114 â†’ 57\n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Block 3: 57 â†’ 28\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # 57 â†’ 28\n",
    "            nn.Dropout2d(dropout),\n",
    "\n",
    "            # Block 4: 28 â†’ 14\n",
    "            nn.Conv2d(96, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),  # 28 â†’ 14\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "\n",
    "        # After CNN: (batch, 128, 14, time)\n",
    "        cnn_output_dim = 128 * 14  # 1792\n",
    "\n",
    "        # Onset LSTM\n",
    "        self.onset_lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Frame LSTM (conditioned on onset predictions)\n",
    "        self.frame_lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim + num_keys,  # CNN features + onset_pred\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        lstm_output_dim = hidden_size * 2  # bidirectional â†’ 512\n",
    "\n",
    "        # Output heads\n",
    "        self.onset_head = nn.Linear(lstm_output_dim, num_keys)\n",
    "        self.frame_head = nn.Linear(lstm_output_dim, num_keys)\n",
    "        self.velocity_head = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, num_keys),\n",
    "            nn.Sigmoid(),  # velocity is 0-1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, n_mels, time) â€” mel spectrogram\n",
    "        Returns:\n",
    "            onset:    (batch, 88, time)\n",
    "            frame:    (batch, 88, time)\n",
    "            velocity: (batch, 88, time)\n",
    "        \"\"\"\n",
    "        batch, n_mels, time = x.shape\n",
    "\n",
    "        # CNN: (batch, 1, n_mels, time) â†’ (batch, 128, 14, time)\n",
    "        cnn_out = self.cnn(x.unsqueeze(1))\n",
    "\n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        cnn_features = cnn_out.permute(0, 3, 1, 2)  # (B, T, 128, 14)\n",
    "        cnn_features = cnn_features.reshape(batch, time, -1)  # (B, T, 1792)\n",
    "\n",
    "        # Onset prediction\n",
    "        onset_out, _ = self.onset_lstm(cnn_features)\n",
    "        onset_logits = self.onset_head(onset_out)      # (B, T, 88)\n",
    "        onset_pred = torch.sigmoid(onset_logits)\n",
    "\n",
    "        # Frame prediction (conditioned on onset)\n",
    "        frame_input = torch.cat([cnn_features, onset_pred.detach()], dim=-1)\n",
    "        frame_out, _ = self.frame_lstm(frame_input)\n",
    "        frame_logits = self.frame_head(frame_out)      # (B, T, 88)\n",
    "        frame_pred = torch.sigmoid(frame_logits)\n",
    "\n",
    "        # Velocity prediction\n",
    "        velocity_pred = self.velocity_head(onset_out)   # (B, T, 88)\n",
    "\n",
    "        # Transpose to (batch, keys, time)\n",
    "        return (\n",
    "            onset_pred.permute(0, 2, 1),\n",
    "            frame_pred.permute(0, 2, 1),\n",
    "            velocity_pred.permute(0, 2, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "# Test model\n",
    "model = PianoTranscriptionModel()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {total_params:,}')\n",
    "\n",
    "# Dummy forward pass\n",
    "dummy = torch.randn(2, N_MELS, CHUNK_FRAMES)\n",
    "onset, frame, vel = model(dummy)\n",
    "print(f'Input:    {dummy.shape}')\n",
    "print(f'Onset:    {onset.shape}')    # (2, 88, 312)\n",
    "print(f'Frame:    {frame.shape}')    # (2, 88, 312)\n",
    "print(f'Velocity: {vel.shape}')      # (2, 88, 312)\n",
    "assert onset.shape == (2, NUM_KEYS, CHUNK_FRAMES)\n",
    "print('\\nâœ… Model architecture verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Training on: {device}')\n\nmodel = PianoTranscriptionModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', patience=5, factor=0.5\n)\n\nonset_loss_fn = nn.BCELoss()\nframe_loss_fn = nn.BCELoss()\nvelocity_loss_fn = nn.MSELoss()\n\n# Resume from checkpoint\nstart_epoch = 0\nbest_val_loss = float('inf')\n\ncheckpoint_files = sorted(Path(DRIVE_OUTPUT).glob('checkpoint_epoch_*.pt'))\nbest_model_path = Path(DRIVE_OUTPUT) / 'best_piano_model.pt'\n\nif checkpoint_files:\n    latest = checkpoint_files[-1]\n    print(f'Resuming from {latest.name}...')\n    ckpt = torch.load(latest, map_location=device, weights_only=False)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    start_epoch = ckpt['epoch'] + 1\n    best_val_loss = ckpt.get('val_loss', float('inf'))\n    print(f'Resumed at epoch {start_epoch}, best_val_loss={best_val_loss:.5f}')\nelif best_model_path.exists():\n    ckpt = torch.load(best_model_path, map_location=device, weights_only=False)\n    model.load_state_dict(ckpt['model_state_dict'])\n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    start_epoch = ckpt['epoch'] + 1\n    best_val_loss = ckpt.get('val_loss', float('inf'))\n    print(f'Resumed at epoch {start_epoch}, best_val_loss={best_val_loss:.5f}')\n\nprint(f'\\nStarting training from epoch {start_epoch}...')\nprint(f'Epochs: {CONFIG[\"num_epochs\"]}, Batch size: {CONFIG[\"batch_size\"]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "NUM_EPOCHS = CONFIG['num_epochs']\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{NUM_EPOCHS}')\n",
    "    for mel_batch, onset_tgt, frame_tgt, vel_tgt in pbar:\n",
    "        mel_batch = mel_batch.to(device)\n",
    "        onset_tgt = onset_tgt.to(device)\n",
    "        frame_tgt = frame_tgt.to(device)\n",
    "        vel_tgt = vel_tgt.to(device)\n",
    "\n",
    "        onset_pred, frame_pred, vel_pred = model(mel_batch)\n",
    "\n",
    "        loss_onset = onset_loss_fn(onset_pred, onset_tgt)\n",
    "        loss_frame = frame_loss_fn(frame_pred, frame_tgt)\n",
    "\n",
    "        # Velocity loss only where onsets exist\n",
    "        onset_mask = onset_tgt > 0.5\n",
    "        if onset_mask.any():\n",
    "            loss_vel = velocity_loss_fn(\n",
    "                vel_pred[onset_mask], vel_tgt[onset_mask]\n",
    "            )\n",
    "        else:\n",
    "            loss_vel = torch.tensor(0.0, device=device)\n",
    "\n",
    "        loss = loss_onset + loss_frame + 0.5 * loss_vel\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "\n",
    "    train_loss = running_loss / max(num_batches, 1)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running = 0.0\n",
    "    val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_batch, onset_tgt, frame_tgt, vel_tgt in val_loader:\n",
    "            mel_batch = mel_batch.to(device)\n",
    "            onset_tgt = onset_tgt.to(device)\n",
    "            frame_tgt = frame_tgt.to(device)\n",
    "            vel_tgt = vel_tgt.to(device)\n",
    "\n",
    "            onset_pred, frame_pred, vel_pred = model(mel_batch)\n",
    "\n",
    "            loss_onset = onset_loss_fn(onset_pred, onset_tgt)\n",
    "            loss_frame = frame_loss_fn(frame_pred, frame_tgt)\n",
    "\n",
    "            onset_mask = onset_tgt > 0.5\n",
    "            if onset_mask.any():\n",
    "                loss_vel = velocity_loss_fn(\n",
    "                    vel_pred[onset_mask], vel_tgt[onset_mask]\n",
    "                )\n",
    "            else:\n",
    "                loss_vel = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = loss_onset + loss_frame + 0.5 * loss_vel\n",
    "            val_running += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    val_loss = val_running / max(val_batches, 1)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    elapsed = time.time() - epoch_start\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f'Epoch {epoch}/{NUM_EPOCHS} | '\n",
    "          f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | '\n",
    "          f'LR: {lr:.2e} | Time: {elapsed:.0f}s')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, str(best_model_path))\n",
    "        print(f'  âœ… Best model saved (val_loss={val_loss:.5f})')\n",
    "\n",
    "    # Periodic checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_path = Path(DRIVE_OUTPUT) / f'checkpoint_epoch_{epoch:03d}.pt'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, str(ckpt_path))\n",
    "        print(f'  ðŸ’¾ Checkpoint saved: {ckpt_path.name}')\n",
    "\n",
    "print(f'\\nðŸŽ‰ Training complete! Best val_loss: {best_val_loss:.5f}')\n",
    "print(f'Best model saved to: {best_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss', color='steelblue')\n",
    "ax.plot(val_losses, label='Val Loss', color='coral')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title(f'Piano Transcription Training (best val_loss={best_val_loss:.5f})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_OUTPUT}/training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model and evaluate\nckpt = torch.load(str(best_model_path), map_location=device, weights_only=False)\nmodel.load_state_dict(ckpt['model_state_dict'])\nmodel.eval()\n\nprint(f'Loaded best model from epoch {ckpt[\"epoch\"]}, val_loss={ckpt[\"val_loss\"]:.5f}')\nprint(f'Config: {ckpt[\"config\"]}')\n\n# Quick inference test\nwith torch.no_grad():\n    test_input = torch.randn(1, N_MELS, CHUNK_FRAMES).to(device)\n    onset, frame, vel = model(test_input)\n    print(f'\\nInference test:')\n    print(f'  Input:    {test_input.shape}')\n    print(f'  Onset:    {onset.shape}, range [{onset.min():.3f}, {onset.max():.3f}]')\n    print(f'  Frame:    {frame.shape}, range [{frame.min():.3f}, {frame.max():.3f}]')\n    print(f'  Velocity: {vel.shape}, range [{vel.min():.3f}, {vel.max():.3f}]')\n\nprint(f'\\nâœ… Model ready for deployment!')\nprint(f'Copy {best_model_path} to backend/models/pretrained/best_piano_model.pt')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}